<!DOCTYPE html><html><head><meta charSet="utf-8"/><title class="jsx-3743936066 jsx-2167767515">Scaling a Lean Startup, The Story So Far.  – Blog – Tom Moor</title><link rel="shortcut icon" href="//www.gravatar.com/avatar/166e0b975c36bbe15caa65209940035c.png" class="jsx-3743936066 jsx-2167767515"/><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/base-min.css" class="jsx-3743936066 jsx-2167767515"/><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-min.css" class="jsx-3743936066 jsx-2167767515"/><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" class="jsx-3743936066 jsx-2167767515"/><link rel="alternate" type="application/rss+xml" title="RSS" href="/rss.xml" class="jsx-3743936066 jsx-2167767515"/><meta name="theme-color" content="#FFFFFF" class="jsx-3743936066 jsx-2167767515"/><meta name="viewport" content="width=device-width, initial-scale=1" class="jsx-3743936066 jsx-2167767515"/><meta name="referrer" content="origin" class="jsx-3743936066 jsx-2167767515"/><meta name="site_name" property="og:site_name" content="Tom Moor" class="jsx-3743936066 jsx-2167767515"/><meta name="type" property="og:type" content="website" class="jsx-3743936066 jsx-2167767515"/><meta name="title" property="og:title" content="Scaling a Lean Startup, The Story So Far.  – Blog" class="jsx-3743936066 jsx-2167767515"/><meta name="twitter:card" content="summary" class="jsx-3743936066 jsx-2167767515"/><meta name="twitter:site" content="@tommoor" class="jsx-3743936066 jsx-2167767515"/><meta name="twitter:domain" content="tommoor.com" class="jsx-3743936066 jsx-2167767515"/><meta name="twitter:title" content="Scaling a Lean Startup, The Story So Far.  – Blog" class="jsx-3743936066 jsx-2167767515"/><meta name="next-head-count" content="17"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-6d72c6ed51d58793f029.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-c617d96353df8716a680.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cdc4828ff8eff3992bf8.js" defer=""></script><script src="/_next/static/chunks/746-66d53be612b6ba6e4a01.js" defer=""></script><script src="/_next/static/chunks/605-05b193d3f94727107b4d.js" defer=""></script><script src="/_next/static/chunks/507-7ae5d7b004bc695011e7.js" defer=""></script><script src="/_next/static/chunks/42-b3b6ee18b61f9d187f90.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5B...slug%5D-a1933e2289039879305f.js" defer=""></script><script src="/_next/static/cjRRrZNmQgC0lC6cnysIB/_buildManifest.js" defer=""></script><script src="/_next/static/cjRRrZNmQgC0lC6cnysIB/_ssgManifest.js" defer=""></script><style id="__jsx-4159018833">a.jsx-4159018833{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:8px 0;margin:0 16px;color:rgba(0,0,0,0.75);-webkit-text-decoration:none;text-decoration:none;white-space:nowrap;min-height:40px;border-bottom:2px solid transparent;font-weight:500;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;overflow:hidden;-webkit-transition:all 100ms ease-in-out;transition:all 100ms ease-in-out;}a.jsx-4159018833:hover{border-bottom:2px solid #328aff;color:#328aff;}</style><style id="__jsx-2216165645">nav.jsx-2216165645{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}ul.jsx-2216165645{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;list-style:none;margin:0;padding:0;}li.jsx-2216165645{display:block;position:relative;margin:0 0 0 16px;}li.jsx-2216165645:hover{cursor:pointer;}li.hidden-on-desktop.jsx-2216165645{display:none;}@media (max-width:48em){li.jsx-2216165645{margin:0 0 0 8px;}li.hidden-on-desktop.jsx-2216165645{display:block;}li.hidden-on-mobile.jsx-2216165645{display:none;}}</style><style id="__jsx-1568871464">.metadata.jsx-1568871464{font-family:Roboto Mono, Menlo, monospace;color:#5E6573;font-size:0.9em;margin-top:-1em;margin-bottom:2em;}.metadata.jsx-1568871464 a.jsx-1568871464{color:#5E6573;}.metadata.jsx-1568871464 a.jsx-1568871464:hover{color:#181A1B;-webkit-text-decoration:underline;text-decoration:underline;}</style><style id="__jsx-1234508262">.md.jsx-1234508262{font-size:1.1em;line-height:1.4;color:#181A1B;}.md.jsx-1234508262 blockquote{margin-left:0;margin-right:0;background-color:#f2f2f2;border-left:6px solid #f2f2f2;padding:15px 30px 15px 15px;font-style:italic;font-size:16px;}.md.jsx-1234508262 a:hover{-webkit-text-decoration:underline;text-decoration:underline;}.md.jsx-1234508262 blockquote p{margin:0;}.md.jsx-1234508262 img{display:block;max-width:100%;box-shadow:0 0 0 1px rgba(0,0,0,0.2);border-radius:8px;margin:2em auto;}.md.jsx-1234508262 img[title="@2x"]{zoom:50%;}.md.jsx-1234508262 code{font-size:15px;background:#f2f2f2;padding:2px 4px;border-radius:2px;}.md.jsx-1234508262 li{line-height:1.6;}.md.jsx-1234508262 h3{margin-top:1.5em;}</style><style id="__jsx-3743936066">header.jsx-3743936066{color:inherit;background:transparent;}.back.jsx-3743936066{position:relative;top:2px;}.header-left.jsx-3743936066,.header-right.jsx-3743936066{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:32px 0;}.header-right.jsx-3743936066{-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}.with-header.jsx-3743936066{padding-bottom:1em;margin-bottom:2em;}.page.jsx-3743936066{min-height:calc(100vh - 300px);max-width:700px;margin:0 auto;}.content.jsx-3743936066{padding:0 32px 48px;}@media (max-width:48em){.content.jsx-3743936066{padding:0 16px;}.back-link.jsx-3743936066{display:none;}}</style><style id="__jsx-2167767515">@font-face{font-family:"HK Grotesk";src:url("/fonts/HKGrotesk-Light.eot") format("eot"), url("/fonts/HKGrotesk-Light.woff2") format("woff2"), url("/fonts/HKGrotesk-Light.woff") format("woff");font-weight:300;font-style:normal;}@font-face{font-family:"HK Grotesk";src:url("/fonts/HKGrotesk-Regular.eot") format("eot"), url("/fonts/HKGrotesk-Regular.woff2") format("woff2"), url("/fonts/HKGrotesk-Regular.woff") format("woff");font-weight:400;font-style:normal;}@font-face{font-family:"HK Grotesk";src:url("/fonts/HKGrotesk-Bold.eot") format("eot"), url("/fonts/HKGrotesk-Bold.woff2") format("woff2"), url("/fonts/HKGrotesk-Bold.woff") format("woff");font-weight:600;font-style:normal;}.container{max-width:1140px;width:90vw;margin:0 auto;}*{box-sizing:border-box;}html,button,input,select,textarea,.pure-g [class*="pure-u"]{color:#121212;font-family:-apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Oxygen, Ubuntu, Cantarell, Fira Sans, Droid Sans, Helvetica Neue, sans-serif;}html,body{padding:0;margin:0;line-height:1.6;}h1{font-size:2em;}h2{font-size:1.2em;margin-top:1.4em;}h1,h2,h3,h4,.pure-g h1[class*="pure-u"],.pure-g h2[class*="pure-u"],.pure-g h3[class*="pure-u"],.pure-g h4[class*="pure-u"]{font-family:"HK Grotesk";font-weight:600;line-height:1;}a{color:#328aff;-webkit-text-decoration:none;text-decoration:none;}p{line-height:1.4;}</style></head><body><div id="__next"><header class="jsx-3743936066 jsx-2167767515 "><div class="jsx-3743936066 jsx-2167767515 container"><div class="jsx-3743936066 jsx-2167767515 pure-g"><div class="jsx-3743936066 jsx-2167767515 pure-u-1-2 header-left"><div class="jsx-3743936066 jsx-2167767515 back-link"><a href="/posts" class="jsx-4159018833 "><span class="jsx-3743936066 jsx-2167767515 back">↩</span> Back</a></div></div><div class="jsx-3743936066 jsx-2167767515 pure-u-1-2 header-right"><nav role="navigation" class="jsx-2216165645"><ul class="jsx-2216165645"><li class="jsx-2216165645"><a href="/" class="jsx-4159018833 ">About</a></li><li class="jsx-2216165645"><a href="/posts" class="jsx-4159018833 ">Blog</a></li><li class="jsx-2216165645 hidden-on-mobile"><a href="/rss.xml" class="jsx-4159018833 ">RSS</a></li><li class="jsx-2216165645"><a href="https://www.twitter.com/tommoor" target="_blank" class="jsx-4159018833 ">Twitter</a></li></ul></nav></div></div></div></header><div class="jsx-3743936066 jsx-2167767515 page"><div class="jsx-3743936066 jsx-2167767515 content"><h1>Scaling a Lean Startup, The Story So Far. </h1><div class="jsx-1568871464 metadata"><time dateTime="2012-06-17T08:00:00.000Z" class="time">June 17th, 2012</time> </div><div class="jsx-1234508262 md"><p>Since I joined <a href="http://bufferapp.com">Buffer</a> in September last year we have seen awesome growth that has pushed both our hardware and mine and <a href="http://twitter.com/joelgascoigne">Joel</a>&#x27;s sys-admin skills to their limits. In this time we have grown from a single Linode VPS shared with Joel&#x27;s previous startup to a heady total of 10 VPS&#x27;s and in the last week we undertook the biggest jump so far, moving our infrastructure from this cluster to <a href="https://aws.amazon.com">Amazon Web Services</a>.</p>
<p>Our approach to scaling has always been based on necessity - with the <a href="http://www.startuplessonslearned.com/2008/09/lean-startup.html">Lean Startup</a> methodology ingrained into the company culture we push code fast and try to avoid prematurely scaling our infrastructure beyond it’s needs. This has definitely lead to some sleepless nights and the occasional hour of downtime but all in all we feel this approach is much better than over compensating from the beginning and building architecture that you may never need or use. These are a few of the key occasions when scaling became the only option…</p>
<h3>1 Server: Launch to 20,000 users</h3>
<p>The first version of Buffer was (<a href="http://blog.bufferapp.com/idea-to-paying-customers-in-7-weeks-how-we-did-it">perhaps famously</a>) the embodiment of an MVP, a single pricing page that gauged potential interest in the idea before anything was even built. This was hosted on a tiny Linode VPS, shared with several other projects. Once the idea was validated and for the next year Joel was able to scale Buffer to 20,000 users by increasing the size of the VPS and optimising SQL queries, this meant a few hours of downtime each time the server was resized but allowed costs and time investment to remain low while product market fit was reached, or in Joel’s words:</p>
<blockquote>
  <p>Focus on “scaling” too early and you may well forget to focus on “building something people want”. Don’t make that mistake.</p>
</blockquote>
<h3>2 Servers: Moving to MongoDB</h3>
<p>As I joined Buffer the first feature we decided to build was support for posting to Facebook - the main decision was how to change the code and database schema to accommodate more than just tweets. Over the course of several weeks the model layer was rewritten and the data migrated table by table from MySQL to collections in the document-based datastore <a href="http://mongodb.org">MongoDB</a>.</p>
<p>After the switchover, the new database was kept on the existing server, taking the place of MySQL. This worked well for around a month until early one Sunday morning Buffer suddenly disappeared from the internet. After tailing the error logs we quickly realised that we had reached an inherent limitation of MongoDB - 32-bit processes are limited to ~2gb of data and we had hit that limit.</p>
<p>We searched frantically for a way to get the database back in action on the existing hardware. After half an hour with no luck and <a href="http://twitter.com/leowid">Leo</a> calming distressed users on twitter we realised we had to get in touch with the best person we knew. We called <a href="http://twitter.com/elubow">Eric Lubow</a> from <a href="http://simplereach.com">SimpleReach</a> who put us on the right path in minutes - there was no other option but to accept the downtime and spin up another 64-bit server immediately. Thanks to clear, straight forward advice we got off very lightly with less than two hours of disruption in a quiet period as we setup a server, tarballed the existing data directory and shuttled it across!</p>
<h3>4 Servers: Background Processes</h3>
<p>By November 2011, Buffer had continued to grow to around 70,000 users and with 20,000 tweets per day being sent the website and browser extension were becoming noticeably slower at the peak time of the day (for us this is around 17:00 GMT, early evening in the UK and morning on the East Coast). At this point it was time to queue the sending tasks and have them processed by their own dedicated server. In hindsight this was the turning point where a Buffer ‘architecture’ started to emerge, with modular pieces of code and servers that we’re independent of each other.</p>
<p>As soon as the core task of sending tweets was moved to queues it became obvious that a lot of other processing could be completed this way such as gathering link info, analytics and image processing. We went ahead and moved these tasks to two new utility servers, I’ll be covering the details behind how we now use queues extensively at Buffer in a future post.</p>
<h3>5 Servers: Upgrading MongoDB</h3>
<p>In December our database server was humming along nicely but quickly filling up it’s available RAM headroom and running on an out of date version of MongoDB. In order to upgrade the software and the server to more RAM (as we would end up doing several times of the next few months) we needed to be in a replica set configuration that would allow us to take the individual database servers out of circulation one by one to upgrade them.</p>
<p>This proved to be another rocky period with several hours of downtime, we found this was almost entirely caused by issues in the immature PHP drivers which have since been addressed but also partly down to our own lack of experience managing replica sets, firewalls and all of the intricacies of a multi-server setup.</p>
<h3>7 Servers: DiggDigg Acquisition</h3>
<p>In January of this year we hit the 100,000 user milestone and completed our acquisition of the popular WordPress plugin <a href="http://bufferapp.com/diggdigg">DiggDigg</a> which allows any WordPress user to quickly add a sharing bar to their blog or website. Part of the plan behind this move was to add the <a href="http://bufferapp.com/extras/button">Buffer Button</a> to DiggDigg and have it enabled by default for new installations, adding massively to the button’s reach and Buffer branding around the web.</p>
<p>In order to handle the millions of new button impressions and isolate the web interface from load generated by third party websites it was time to move the button to it’s own servers and introduce our first load balancer. It was also decided to use Nginx AND just for good measure re-write the entire button to become independent of the Buffer framework.</p>
<p>We definitely teetered on the edge of our seats after deploying all of this previously unused architecture at once, waiting for the load from DiggDigg to hit the servers. But everything went smoothly, the load balancer acted as expected and impressions gradually rose to new highs as people upgraded their plugin (it seems there is no sudden rush of traffic!). The button servers now handle around 4 million impressions per day and rising!</p>
<h3>9 Servers: Sleepless Nights</h3>
<p>Last month we reached 180,000 registered users and completed the acquisition of ShareFeed from <a href="http://www.kissmetrics.com">KISSmetrics</a>. Almost unbelievably, at this time the main website, API, blog and browser extension were all still running from a single Linode VPS (albeit one that had been upgraded on many occasions!). During peak times things we’re beginning to feel slow again and complaints started to appear on Twitter at predictable times.</p>
<p>Unfortunately late evening Hong Kong time (<a href="http://techcrunch.com/2011/12/20/sharing-scheduler-app-buffer-raises-400000-gets-kicked-out-of-us">where we are currently based</a>) is the peak time for traffic at Buffer. For almost a week I had incredibly restless sleep, I became acutely aware of every email received, waking up at the slightest sound convinced that the site would be down. It was definitely time to upgrade.</p>
<p>Thanks to all we had learnt creating the new environment for the Buttons several months before, we were able to make this move completely seamlessly. By setting up the load balancer, adding the new servers and testing thoroughly before moving the DNS we were able to ensure that the only thing users noticed was a performance increase.</p>
<h3>Thats Not All Folks</h3>
<p>It’s been an amazing ride so far with both highs and some serious lows, at one point working for nearly 30 hours straight just to keep the site online. We have learnt an incredible amount as a team in the last few months and I can’t think of any way we could have learnt more, faster.</p>
<p>Last week we migrated the whole of Buffer’s architecture to Amazon Web Services with less than two hours of scheduled downtime, making a number of huge changes and improvements in the process. I’ll cover some details of this migration in a future post.</p>
<p><strong>I’d love to hear any comments your have or similar stories of battling scaling woes as a small startup!</strong></p></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"Scaling a Lean Startup, The Story So Far. ","slug":"2012/06/17/scaling-a-lean-startup-the-story-so-far","date":"2012-06-17T08:00:00.000Z","tag":"","content":"\n\u003cp\u003eSince I joined \u003ca href=\"http://bufferapp.com\"\u003eBuffer\u003c/a\u003e in September last year we have seen awesome growth that has pushed both our hardware and mine and \u003ca href=\"http://twitter.com/joelgascoigne\"\u003eJoel\u003c/a\u003e's sys-admin skills to their limits. In this time we have grown from a single Linode VPS shared with Joel's previous startup to a heady total of 10 VPS's and in the last week we undertook the biggest jump so far, moving our infrastructure from this cluster to \u003ca href=\"https://aws.amazon.com\"\u003eAmazon Web Services\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eOur approach to scaling has always been based on necessity - with the \u003ca href=\"http://www.startuplessonslearned.com/2008/09/lean-startup.html\"\u003eLean Startup\u003c/a\u003e methodology ingrained into the company culture we push code fast and try to avoid prematurely scaling our infrastructure beyond it\u0026#8217;s needs. This has definitely lead to some sleepless nights and the occasional hour of downtime but all in all we feel this approach is much better than over compensating from the beginning and building architecture that you may never need or use. These are a few of the key occasions when scaling became the only option\u0026#8230;\u003c/p\u003e\n\n\u003ch3\u003e1 Server: Launch to 20,000 users\u003c/h3\u003e\n\n\u003cp\u003eThe first version of Buffer was (\u003ca href=\"http://blog.bufferapp.com/idea-to-paying-customers-in-7-weeks-how-we-did-it\"\u003eperhaps famously\u003c/a\u003e) the embodiment of an MVP, a single pricing page that gauged potential interest in the idea before anything was even built. This was hosted on a tiny Linode VPS, shared with several other projects. Once the idea was validated and for the next year Joel was able to scale Buffer to 20,000 users by increasing the size of the VPS and optimising SQL queries, this meant a few hours of downtime each time the server was resized but allowed costs and time investment to remain low while product market fit was reached, or in Joel\u0026#8217;s words:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eFocus on \u0026#8220;scaling\u0026#8221; too early and you may well forget to focus on \u0026#8220;building something people want\u0026#8221;. Don\u0026#8217;t make that mistake.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3\u003e2 Servers: Moving to MongoDB\u003c/h3\u003e\n\n\u003cp\u003eAs I joined Buffer the first feature we decided to build was support for posting to Facebook - the main decision was how to change the code and database schema to accommodate more than just tweets. Over the course of several weeks the model layer was rewritten and the data migrated table by table from MySQL to collections in the document-based datastore \u003ca href=\"http://mongodb.org\"\u003eMongoDB\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAfter the switchover, the new database was kept on the existing server, taking the place of MySQL. This worked well for around a month until early one Sunday morning Buffer suddenly disappeared from the internet. After tailing the error logs we quickly realised that we had reached an inherent limitation of MongoDB - 32-bit processes are limited to ~2gb of data and we had hit that limit.\u003c/p\u003e\n\n\u003cp\u003eWe searched frantically for a way to get the database back in action on the existing hardware. After half an hour with no luck and \u003ca href=\"http://twitter.com/leowid\"\u003eLeo\u003c/a\u003e calming distressed users on twitter we realised we had to get in touch with the best person we knew. We called \u003ca href=\"http://twitter.com/elubow\"\u003eEric Lubow\u003c/a\u003e from \u003ca href=\"http://simplereach.com\"\u003eSimpleReach\u003c/a\u003e who put us on the right path in minutes - there was no other option but to accept the downtime and spin up another 64-bit server immediately. Thanks to clear, straight forward advice we got off very lightly with less than two hours of disruption in a quiet period as we setup a server, tarballed the existing data directory and shuttled it across!\u003c/p\u003e\n\n\u003ch3\u003e4 Servers: Background Processes\u003c/h3\u003e\n\n\u003cp\u003eBy November 2011, Buffer had continued to grow to around 70,000 users and with 20,000 tweets per day being sent the website and browser extension were becoming noticeably slower at the peak time of the day (for us this is around 17:00 GMT, early evening in the UK and morning on the East Coast). At this point it was time to queue the sending tasks and have them processed by their own dedicated server. In hindsight this was the turning point where a Buffer \u0026#8216;architecture\u0026#8217; started to emerge, with modular pieces of code and servers that we\u0026#8217;re independent of each other.\u003c/p\u003e\n\n\u003cp\u003eAs soon as the core task of sending tweets was moved to queues it became obvious that a lot of other processing could be completed this way such as gathering link info, analytics and image processing. We went ahead and moved these tasks to two new utility servers, I\u0026#8217;ll be covering the details behind how we now use queues extensively at Buffer in a future post.\u003c/p\u003e\n\n\u003ch3\u003e5 Servers: Upgrading MongoDB\u003c/h3\u003e\n\n\u003cp\u003eIn December our database server was humming along nicely but quickly filling up it\u0026#8217;s available RAM headroom and running on an out of date version of MongoDB. In order to upgrade the software and the server to more RAM (as we would end up doing several times of the next few months) we needed to be in a replica set configuration that would allow us to take the individual database servers out of circulation one by one to upgrade them.\u003c/p\u003e\n\n\u003cp\u003eThis proved to be another rocky period with several hours of downtime, we found this was almost entirely caused by issues in the immature PHP drivers which have since been addressed but also partly down to our own lack of experience managing replica sets, firewalls and all of the intricacies of a multi-server setup.\u003c/p\u003e\n\n\u003ch3\u003e7 Servers: DiggDigg Acquisition\u003c/h3\u003e\n\n\u003cp\u003eIn January of this year we hit the 100,000 user milestone and completed our acquisition of the popular WordPress plugin \u003ca href=\"http://bufferapp.com/diggdigg\"\u003eDiggDigg\u003c/a\u003e which allows any WordPress user to quickly add a sharing bar to their blog or website. Part of the plan behind this move was to add the \u003ca href=\"http://bufferapp.com/extras/button\"\u003eBuffer Button\u003c/a\u003e to DiggDigg and have it enabled by default for new installations, adding massively to the button\u0026#8217;s reach and Buffer branding around the web.\u003c/p\u003e\n\n\u003cp\u003eIn order to handle the millions of new button impressions and isolate the web interface from load generated by third party websites it was time to move the button to it\u0026#8217;s own servers and introduce our first load balancer. It was also decided to use Nginx AND just for good measure re-write the entire button to become independent of the Buffer framework.\u003c/p\u003e\n\n\u003cp\u003eWe definitely teetered on the edge of our seats after deploying all of this previously unused architecture at once, waiting for the load from DiggDigg to hit the servers. But everything went smoothly, the load balancer acted as expected and impressions gradually rose to new highs as people upgraded their plugin (it seems there is no sudden rush of traffic!). The button servers now handle around 4 million impressions per day and rising!\u003c/p\u003e\n\n\u003ch3\u003e9 Servers: Sleepless Nights\u003c/h3\u003e\n\n\u003cp\u003eLast month we reached 180,000 registered users and completed the acquisition of ShareFeed from \u003ca href=\"http://www.kissmetrics.com\"\u003eKISSmetrics\u003c/a\u003e. Almost unbelievably, at this time the main website, API, blog and browser extension were all still running from a single Linode VPS (albeit one that had been upgraded on many occasions!). During peak times things we\u0026#8217;re beginning to feel slow again and complaints started to appear on Twitter at predictable times.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately late evening Hong Kong time (\u003ca href=\"http://techcrunch.com/2011/12/20/sharing-scheduler-app-buffer-raises-400000-gets-kicked-out-of-us\"\u003ewhere we are currently based\u003c/a\u003e) is the peak time for traffic at Buffer. For almost a week I had incredibly restless sleep, I became acutely aware of every email received, waking up at the slightest sound convinced that the site would be down. It was definitely time to upgrade.\u003c/p\u003e\n\n\u003cp\u003eThanks to all we had learnt creating the new environment for the Buttons several months before, we were able to make this move completely seamlessly. By setting up the load balancer, adding the new servers and testing thoroughly before moving the DNS we were able to ensure that the only thing users noticed was a performance increase.\u003c/p\u003e\n\n\u003ch3\u003eThats Not All Folks\u003c/h3\u003e\n\n\u003cp\u003eIt\u0026#8217;s been an amazing ride so far with both highs and some serious lows, at one point working for nearly 30 hours straight just to keep the site online. We have learnt an incredible amount as a team in the last few months and I can\u0026#8217;t think of any way we could have learnt more, faster.\u003c/p\u003e\n\n\u003cp\u003eLast week we migrated the whole of Buffer\u0026#8217;s architecture to Amazon Web Services with less than two hours of scheduled downtime, making a number of huge changes and improvements in the process. I\u0026#8217;ll cover some details of this migration in a future post.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eI\u0026#8217;d love to hear any comments your have or similar stories of battling scaling woes as a small startup!\u003c/strong\u003e\u003c/p\u003e"},"__N_SSG":true},"page":"/posts/[...slug]","query":{"slug":["2012","06","17","scaling-a-lean-startup-the-story-so-far"]},"buildId":"cjRRrZNmQgC0lC6cnysIB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>